{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier, EasyEnsembleClassifier, BalancedBaggingClassifier\n",
    "from imblearn.over_sampling import SMOTE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv(\"D:\\MSIS\\Customer-Churn-Prediction---Using-TensorFlow\\Data\\WA_Fn-UseC_-Telco-Customer-Churn.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "data['TotalCharges'] = pd.to_numeric(data.TotalCharges, errors='coerce')\n",
    "data.drop(labels=data[data['tenure'] == 0].index, axis=0, inplace=True)\n",
    "data.fillna(data[\"TotalCharges\"].mean(), inplace=True)\n",
    "data = data.drop(['customerID'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Transforming object to int using Label Encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "def object_to_int(dataframe_series):\n",
    "    if dataframe_series.dtype == 'object':\n",
    "        dataframe_series = encoder.fit_transform(dataframe_series)\n",
    "    return dataframe_series\n",
    "\n",
    "data = data.apply(lambda x: object_to_int(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data splitting\n",
    "X = data.drop('Churn', axis=1)\n",
    "y = data['Churn'].values\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'smote = SMOTE(random_state=42)\\nX_train, y_train = smote.fit_resample(X_train, y_train)'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Handle imbalance using SMOTE (oversampling the minority class)\n",
    "'''smote = SMOTE(random_state=42)\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from imblearn.under_sampling import RandomUnderSampler\\nrus = RandomUnderSampler(random_state=42)\\nX_train, y_train = rus.fit_resample(X_train, y_train)'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from imblearn.under_sampling import RandomUnderSampler\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_train, y_train = rus.fit_resample(X_train, y_train)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Model  Accuracy  F1 Score           Confusion Matrix\n",
      "0     Logistic Regression  0.735545  0.614108  [[1108, 441], [117, 444]]\n",
      "1           Decision Tree  0.739336  0.514134  [[1269, 280], [270, 291]]\n",
      "2           Random Forest  0.781043  0.529532  [[1388, 161], [301, 260]]\n",
      "3  Support Vector Machine  0.727014  0.601108  [[1100, 449], [127, 434]]\n",
      "4       Gradient Boosting  0.794787  0.566567  [[1394, 155], [278, 283]]\n",
      "5                 XGBoost  0.759242  0.589661  [[1237, 312], [196, 365]]\n",
      "6  Balanced Random Forest  0.755450  0.600000  [[1207, 342], [174, 387]]\n",
      "7           Easy Ensemble  0.731280  0.615071  [[1090, 459], [108, 453]]\n",
      "8        Balanced Bagging  0.757346  0.576860  [[1249, 300], [212, 349]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(class_weight='balanced', random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(class_weight='balanced', random_state=42),\n",
    "    'Support Vector Machine': SVC(class_weight='balanced', random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    'XGBoost': XGBClassifier(scale_pos_weight=len(y_train[y_train == 0]) / len(y_train[y_train == 1]), random_state=42),\n",
    "    'Balanced Random Forest': BalancedRandomForestClassifier(random_state=42),\n",
    "    'Easy Ensemble': EasyEnsembleClassifier(random_state=42),\n",
    "    'Balanced Bagging': BalancedBaggingClassifier(estimator=DecisionTreeClassifier(), random_state=42)\n",
    "}\n",
    "\n",
    "# Initialize lists to store results\n",
    "results = []\n",
    "\n",
    "# Evaluate each model\n",
    "for model_name, model in models.items():\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "    \n",
    "    # Fit the model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'F1 Score': f1,\n",
    "        'Confusion Matrix': cm\n",
    "    })\n",
    "\n",
    "# Create a DataFrame to compare the models\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Print the results\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(19, input_shape=(19,), activation='relu'),\n",
    "    keras.layers.Dense(15, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# opt = keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 00m 24s]\n",
      "val_accuracy: 0.7842021981875101\n",
      "\n",
      "Best val_accuracy So Far: 0.790837307771047\n",
      "Total elapsed time: 00h 01m 42s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "c:\\Program Files\\Python311\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m154/154\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.7913 - loss: 0.4363\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7927 - loss: 0.4581\n",
      "Model performance for Training set\n",
      "- Accuracy: 0.7992686033248901\n",
      "- Loss: 0.4305054247379303\n",
      "----------------------------------\n",
      "Model performance for Validation set\n",
      "- Accuracy: 0.7962085604667664\n",
      "- Loss: 0.4447570741176605\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Confusion Matrix:\n",
      " [[1364  185]\n",
      " [ 245  316]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.88      0.86      1549\n",
      "           1       0.63      0.56      0.60       561\n",
      "\n",
      "    accuracy                           0.80      2110\n",
      "   macro avg       0.74      0.72      0.73      2110\n",
      "weighted avg       0.79      0.80      0.79      2110\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.metrics import accuracy_score\n",
    "import keras_tuner\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Page Configuration\n",
    "st.set_page_config(\n",
    "    page_title=\"Customer Churn Prediction\",\n",
    "    page_icon=\"üìä\",\n",
    "    layout=\"wide\",\n",
    ")\n",
    "\n",
    "# Load and preprocess data\n",
    "@st.cache_data\n",
    "def load_data():\n",
    "    data_path = \"Data/WA_Fn-UseC_-Telco-Customer-Churn.csv\"\n",
    "    data = pd.read_csv(data_path)\n",
    "    data['TotalCharges'] = pd.to_numeric(data['TotalCharges'], errors='coerce')\n",
    "    data.dropna(subset=['TotalCharges'], inplace=True)\n",
    "    data[\"SeniorCitizen\"] = data[\"SeniorCitizen\"].map({0: \"No\", 1: \"Yes\"})\n",
    "    data = data.drop(['customerID'], axis=1)\n",
    "\n",
    "    # Encode categorical variables\n",
    "    categorical_cols = data.select_dtypes(include=['object']).columns\n",
    "    data = pd.get_dummies(data, columns=categorical_cols, drop_first=True)\n",
    "    return data\n",
    "\n",
    "data = load_data()\n",
    "\n",
    "# Split data\n",
    "X = data.drop('Churn_Yes', axis=1)\n",
    "y = data['Churn_Yes']\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Hyperparameter tuning with Keras Tuner\n",
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    hp_units1 = hp.Int('units1', min_value=10, max_value=50, step=10)\n",
    "    hp_units2 = hp.Int('units2', min_value=10, max_value=50, step=10)\n",
    "    model.add(layers.Dense(units=hp_units1, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "    model.add(layers.Dense(units=hp_units2, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Tune the learning rate for the optimizer\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Initialize the tuner\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=5,\n",
    "    executions_per_trial=3,\n",
    "    directory='my_dir',\n",
    "    project_name='tuning_example'\n",
    ")\n",
    "\n",
    "# Run the hyperparameter search\n",
    "tuner.search(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n",
    "\n",
    "# Retrieve the best model\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "# Train the best model\n",
    "best_model.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test), verbose=1)\n",
    "\n",
    "# Streamlit app\n",
    "st.title(\"Customer Churn Prediction\")\n",
    "\n",
    "# User input form\n",
    "st.sidebar.header(\"Input Customer Details\")\n",
    "def user_input_features():\n",
    "    features = {}\n",
    "    for col in X.columns:\n",
    "        if data[col].max() <= 1:  # Binary or normalized column\n",
    "            features[col] = st.sidebar.slider(col, 0.0, 1.0, 0.5)\n",
    "        else:\n",
    "            features[col] = st.sidebar.number_input(col, float(data[col].min()), float(data[col].max()), float(data[col].mean()))\n",
    "    return pd.DataFrame(features, index=[0])\n",
    "\n",
    "input_df = user_input_features()\n",
    "\n",
    "# Preprocess user input\n",
    "input_scaled = scaler.transform(input_df)\n",
    "\n",
    "# Make prediction\n",
    "prediction_proba = best_model.predict(input_scaled)[0][0]\n",
    "prediction = \"Yes\" if prediction_proba > 0.5 else \"No\"\n",
    "\n",
    "# Display prediction\n",
    "st.subheader(\"Prediction\")\n",
    "st.write(f\"The customer will churn: **{prediction}**\")\n",
    "\n",
    "st.subheader(\"Prediction Probability\")\n",
    "st.write(f\"Probability of churn: **{prediction_proba:.2f}**\")\n",
    "\n",
    "# Developer details on the right\n",
    "st.markdown(\"\"\"\n",
    "    <style>\n",
    "    .developer-info {\n",
    "        position: fixed;\n",
    "        top: 10px;\n",
    "        right: 10px;\n",
    "        font-size: 18px;\n",
    "        font-weight: bold;\n",
    "        color: #333;\n",
    "    }\n",
    "    </style>\n",
    "    <div class=\"developer-info\">\n",
    "        **Developer:** Akshat Maurya<br>\n",
    "        [GitHub](https://github.com/akshatm13) | [LinkedIn](https://www.linkedin.com/in/makshat13)\n",
    "    </div>\n",
    "\"\"\", unsafe_allow_html=True)\n",
    "\n",
    "# Display model performance\n",
    "st.subheader(\"Model Performance on Test Set\")\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_classes = (y_pred > 0.5).astype(int)\n",
    "\n",
    "# Evaluate the best model\n",
    "train_results = best_model.evaluate(X_train, y_train)\n",
    "val_results = best_model.evaluate(X_test, y_test)\n",
    "st.write(f\"Training Accuracy: {train_results[1]*100:.2f}%\")\n",
    "st.write(f\"Validation Accuracy: {val_results[1]*100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
